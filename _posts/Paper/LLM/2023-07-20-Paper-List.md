---
layout: post
title: LLM 量化
category: Paper
tags: LLM
keywords: LLM
description:
---

### Vision Transformers

| Submitted  | Quantization | Tittle                                                       | Links                                                        | Keys  | Model                  |
| ---------- | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ----- | ---------------------- |
| 2023.07.01 | VVTQ         | Variation-aware Vision Transformer Quantization              | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2307.00331) [![Github stars](https://img.shields.io/github/stars/HuangOwen/VVTQ.svg?style=social&label=Github%20%20Star)](https://github.com/HuangOwen/VVTQ) | `QAT` | `DeiT`、`SRet`、`Swin` |
| 2023.05.21 | Bi-ViT       | Bi-ViT: Pushing the Limit of Vision Transformer Quantization | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2305.12354) |       |                        |
| 2023.05.18 | GPUSQ-ViT    | Boost Vision Transformer with GPU-Friendly Sparsity and Quantization | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2305.10727) |       |                        |
| 2023.05.11 | PMQ          | Patch-wise Mixed-Precision Quantization of Vision Transformer | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2305.06559) |       |                        |
| 2023.03.23 | SQ-ViT       | Scaled Quantization for the Vision Transformer               | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2303.13601) | `PTQ` |                        |
| 2023.03.22 | Q-HyViT      | Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2303.12557) | `PTQ` |                        |
| 2023.02.04 | OFQ          | Oscillation-free Quantization for Low-bit Vision Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2302.02210) [![Github stars](https://img.shields.io/github/stars/nbasyl/OFQ.svg?style=social&label=Github%20%20Star)](https://github.com/nbasyl/OFQ) |       | `DeiT`、`Swin`         |
| 2023.04.01 | Q-DETR       | Q-DETR: An Efficient Low-Bit Quantized Detection Transformer | [![arXiv](https://img.shields.io/badge/arXiv-2023-b31b1b.svg)](https://arxiv.org/abs/2304.00253) |       |                        |
| 2022.12.16 | RepQ-ViT     | RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2022-b31b1b.svg)](https://arxiv.org/abs/2212.08254) | `PTQ` |                        |
| 2022.11.29 | NoisyQuant   | NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2022-b31b1b.svg)](https://arxiv.org/abs/2211.16056) | `PTQ` |                        |
| 2022.11.17 | CPT-V        | CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2022-b31b1b.svg)](https://arxiv.org/abs/2211.09643) |       |                        |
| 2022.10.13 | Q-ViT        | Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer | [![arXiv](https://img.shields.io/badge/NeurIPS-2022-b31b1b.svg)](https://arxiv.org/abs/2210.06707) [![Github stars](https://img.shields.io/github/stars/yanjingli0202/q-vit.svg?style=social&label=Github%20%20Star)](https://github.com/yanjingli0202/q-vit) |       | `DeiT`                 |
| 2022.09.13 | PSAQ-ViT V2  | PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2022-b31b1b.svg)](https://arxiv.org/abs/2209.05687) [![Github stars](https://img.shields.io/github/stars/zkkli/psaq-vit.svg?style=social&label=Github%20%20Star)](https://github.com/zkkli/psaq-vit) |       | `DeiT`、`Swin`         |
| 2022.07.04 | I-ViT        | I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference | [![arXiv](https://img.shields.io/badge/arXiv-2022-b31b1b.svg)](https://arxiv.org/abs/2207.01405) |       |                        |
| 2022.03.04 | PSAQ-ViT     | Patch Similarity Aware Data-Free Quantization for Vision Transformers | [![arXiv](https://img.shields.io/badge/ECCV-2022-b31b1b.svg)](https://arxiv.org/abs/2203.02250) [![Github stars](https://img.shields.io/github/stars/zkkli/psaq-vit.svg?style=social&label=Github%20%20Star)](https://github.com/zkkli/psaq-vit) | `PTQ` | `DeiT`、`Swin`         |
| 2022.01.19 | Q-ViT        | Q-ViT: Fully Differentiable Quantization for Vision Transformer | [![arXiv](https://img.shields.io/badge/arXiv-2022-b31b1b.svg)](https://arxiv.org/abs/2201.07703) |       |                        |
| 2022.10.10 | APQ-ViT      | Towards Accurate Post-Training Quantization for Vision Transformer | [![arXiv](https://img.shields.io/badge/ACMMM-2022-b31b1b.svg)](https://arxiv.org/abs/2303.14341) |       |                        |
| 2021.11.27 | FQ-ViT       | FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer | [![arXiv](https://img.shields.io/badge/IJCAI-2022-b31b1b.svg)](https://arxiv.org/abs/2111.13824) [![Github stars](https://img.shields.io/github/stars/megvii-research/FQ-ViT.svg?style=social&label=Github%20%20Star)](https://github.com/megvii-research/FQ-ViT) | `PTQ` | `DeiT`、`ViT`、`Swin`  |
| 2021.11.24 | PTQ4ViT      | PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization | [![arXiv](https://img.shields.io/badge/ECCV-2022-b31b1b.svg)](https://arxiv.org/abs/2111.12293) [![Github stars](https://img.shields.io/github/stars/hahnyuan/ptq4vit.svg?style=social&label=Github%20%20Star)](https://github.com/hahnyuan/ptq4vit) | `PTQ` | `ViT`、`Deit`、`Swin`  |
| 2021.05.27 | ViT-quant    | Post-Training Quantization for Vision Transformer            | [![arXiv](https://img.shields.io/badge/NeurIPS-2021-b31b1b.svg)](https://arxiv.org/abs/2106.14156) | `PTQ` |                        |

### Language Transformers

| Submitted  | Last Revised | Quantization         | Tittle                                                       | Links                                                        | Keys      |
| ---------- | ------------ | -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- |
| 2023.06.13 |              | SqueezeLLM           | Dense-and-Sparse Quantization                                | [![arXiv](https://img.shields.io/badge/arXiv-2306.07629-b31b1b.svg)](https://arxiv.org/abs/2306.07629) [![Github stars](https://img.shields.io/github/stars/squeezeailab/squeezellm.svg?style=social&label=Github%20%20Star)](https://github.com/squeezeailab/squeezellm) | `PTQ`     |
| 2023.06.05 |              | SpQR                 | A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression | [![arXiv](https://img.shields.io/badge/arXiv-2306.03078-b31b1b.svg)](https://arxiv.org/abs/2306.03078) [![Github stars](https://img.shields.io/github/stars/vahe1994/spqr.svg?style=social&label=Github%20%20Star)](https://github.com/vahe1994/spqr) | `PTQ`     |
| 2023.06.04 | 2023.06.13   | OWQ                  | Lessons learned from activation outliers for weight quantization in large language models | [![arXiv](https://img.shields.io/badge/arXiv-2306.02272-b31b1b.svg)](http://arxiv.org/abs/2306.02272) [![Github stars](https://img.shields.io/github/stars/xvyaward/owq.svg?style=social&label=Github%20%20Star)](https://github.com/xvyaward/owq) | `PTQ`     |
| 2023.06.01 |              | AWQ                  | Activation-aware Weight Quantization for LLM Compression and Acceleration | [![arXiv](https://img.shields.io/badge/arXiv-2306.00978-b31b1b.svg)](https://arxiv.org/abs/2306.00978) [![Github stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq.svg?style=social&label=Github%20%20Star)](https://github.com/mit-han-lab/llm-awq) | `PTQ`     |
| 2023.05.30 |              | PreQuant             | A Task-agnostic Quantization Approach for Pre-trained Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2306.00014-b31b1b.svg)](http://arxiv.org/abs/2306.00014) |           |
| 2023.05.29 |              | LLM-QAT              | Data-Free Quantization Aware Training for Large Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2305.17888-b31b1b.svg)](https://arxiv.org/abs/2305.17888) |           |
| 2023.05.23 |              | QLoRA                | Efficient Finetuning of Quantized LLMs                       | [![arXiv](https://img.shields.io/badge/arXiv-2305.14314-b31b1b.svg)](https://arxiv.org/abs/2305.14314) [![Github stars](https://img.shields.io/github/stars/artidoro/qlora.svg?style=social&label=Github%20%20Star)](https://github.com/artidoro/qlora) | `QAT`     |
| 2023.05.23 |              | PEQA                 | Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization | [![arXiv](https://img.shields.io/badge/arXiv-2305.14152-b31b1b.svg)](https://arxiv.org/abs/2305.14152) |           |
| 2023.05.15 | 2023.05.26   | ZeroQuant-V2         | Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation | [![arXiv](https://img.shields.io/badge/arXiv-2303.08302-b31b1b.svg)](https://arxiv.org/abs/2303.08302) [![Github stars](https://img.shields.io/github/stars/microsoft/DeepSpeed.svg?style=social&label=Github%20%20Star)](https://github.com/microsoft/DeepSpeed) | `PTQ`     |
| 2023.04.18 |              | Outlier Suppression+ | Accurate quantization of large language models by equivalent and optimal shifting and scaling | [![arXiv](https://img.shields.io/badge/arXiv-2304.09145-b31b1b.svg)](https://arxiv.org/abs/2304.09145) | `PTQ`     |
| 2023.04.03 | 2023.05.17   | RPTQ                 | Reorder-based Post-training Quantization for Large Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2304.01089-b31b1b.svg)](https://arxiv.org/abs/2304.01089) [![Github stars](https://img.shields.io/github/stars/hahnyuan/rptq4llm.svg?style=social&label=Github%20%20Star)](https://github.com/hahnyuan/rptq4llm) | `PTQ`     |
| 2022.11.18 | 2023.06.05   | SmoothQuant          | Accurate and Efficient Post-Training Quantization for Large Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2211.10438-b31b1b.svg)](https://arxiv.org/abs/2211.10438) [![Github stars](https://img.shields.io/github/stars/mit-han-lab/smoothquant.svg?style=social&label=Github%20%20Star)](https://github.com/mit-han-lab/smoothquant) | `PTQ`     |
| 2022.10.31 | 2023.03.22   | GPTQ                 | Accurate Post-Training Quantization for Generative Pre-trained Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2210.17323-b31b1b.svg)](https://arxiv.org/abs/2210.17323) [![Github stars](https://img.shields.io/github/stars/ist-daslab/gptq.svg?style=social&label=Github%20%20Star)](https://github.com/ist-daslab/gptq) | `PTQ`     |
| 2022.09.27 | 2023.02.21   | Outlier Suppression  | Pushing the Limit of Low-bit Transformer Language Models     | [![arXiv](https://img.shields.io/badge/arXiv-2209.13325-b31b1b.svg)](https://arxiv.org/abs/2209.13325) [![Github stars](https://img.shields.io/github/stars/wimh966/outlier_suppression.svg?style=social&label=Github%20%20Star)](https://github.com/wimh966/outlier_suppression) | `PTQ`     |
| 2022.08.15 | 2022.11.10   | LLM.int8()           | 8-bit Matrix Multiplication for Transformers at Scale        | [![arXiv](https://img.shields.io/badge/arXiv-2208.07339-b31b1b.svg)](https://arxiv.org/abs/2208.07339) [![Github stars](https://img.shields.io/github/stars/timdettmers/bitsandbytes.svg?style=social&label=Github%20%20Star)](https://github.com/timdettmers/bitsandbytes) |           |
| 2022.06.20 | 2023.04.15   | LUT-GEMM             | Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2206.09557-b31b1b.svg)](https://arxiv.org/abs/2206.09557) |           |
| 2022.06.04 |              | ZeroQuant            | Efficient and Affordable Post-Training Quantization for Large-Scale Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2206.01861-b31b1b.svg)](https://arxiv.org/abs/2206.01861) [![Github stars](https://img.shields.io/github/stars/microsoft/DeepSpeed.svg?style=social&label=Github%20%20Star)](https://github.com/microsoft/DeepSpeed) | `PTQ`     |
| 2022.05.25 | 2022.10.02   | BiT                  | Robustly Binarized Multi-distilled Transformer               | [![arXiv](https://img.shields.io/badge/arXiv-2205.13016-b31b1b.svg)](https://arxiv.org/abs/2205.13016) [![Github stars](https://img.shields.io/github/stars/facebookresearch/bit.svg?style=social&label=Github%20%20Star)](https://github.com/facebookresearch/bit) | `Extreme` |
| 2022.05.21 | 2022.07.16   |                      | Compression of Generative Pre-trained Language Models via Quantization | [![arXiv](https://img.shields.io/badge/arXiv-2203.10705-b31b1b.svg)](https://arxiv.org/abs/2203.10705) |           |
| 2022.03.12 |              | BiBERT               | Accurate Fully Binarized BERT                                | [![arXiv](https://img.shields.io/badge/arXiv-2203.10705-b31b1b.svg)](https://arxiv.org/abs/2203.06390) [![Github stars](https://img.shields.io/github/stars/htqin/BiBERT.svg?style=social&label=Github%20%20Star)](https://github.com/htqin/BiBERT) | `Extreme` |
| 2021.09.30 |              | MREM                 | Towards Efficient Post-training Quantization of Pre-trained Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2109.15082-b31b1b.svg)](https://arxiv.org/abs/2109.15082) | `PTQ`     |
| 2021.09.27 |              | PEG-PTQ              | Understanding and Overcoming the Challenges of Efficient Transformer Quantization | [![arXiv](https://img.shields.io/badge/arXiv-2109.12948-b31b1b.svg)](https://arxiv.org/abs/2109.12948) [![Github stars](https://img.shields.io/github/stars/qualcomm-ai-research/transformer-quantization.svg?style=social&label=Github%20%20Star)](https://github.com/qualcomm-ai-research/transformer-quantization) |           |
| 2021.06.02 |              | SPIQA                | On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2106.01335-b31b1b.svg)](https://arxiv.org/abs/2106.01335) [![Github stars](https://img.shields.io/github/stars/StonyBrookNLP/spiqa.svg?style=social&label=Github%20%20Star)](https://github.com/StonyBrookNLP/spiqa) |           |
| 2021.01.15 |              | KDLSQ-BERT           | A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization | [![arXiv](https://img.shields.io/badge/arXiv-2101.05938-b31b1b.svg)](https://arxiv.org/abs/2101.05938) |           |
| 2021.01.05 | 2021.05.08   | I-BERT               | Integer-only BERT Quantization                               | [![arXiv](https://img.shields.io/badge/arXiv-2101.01321-b31b1b.svg)](https://arxiv.org/abs/2101.01321) [![Github stars](https://img.shields.io/github/stars/kssteven418/I-BERT.svg?style=social&label=Github%20%20Star)](https://github.com/kssteven418/I-BERT) |           |
| 2020.12.31 | 2021.07.22   | BinaryBERT           | Pushing the Limit of BERT Quantization                       | [![arXiv](https://img.shields.io/badge/arXiv-2012.15701-b31b1b.svg)](https://arxiv.org/abs/2012.15701) [![Github stars](https://img.shields.io/github/stars/huawei-noah/Pretrained-Language-Model.svg?style=social&label=Github%20%20Star)](https://github.com/huawei-noah/Pretrained-Language-Model) | `Extreme` |
| 2020.09.27 | 2020.10.10   | TernaryBERT          | Distillation-aware Ultra-low Bit BERT                        | [![arXiv](https://img.shields.io/badge/arXiv-2009.12812-b31b1b.svg)](https://arxiv.org/abs/2009.12812) [![Github stars](https://img.shields.io/github/stars/huawei-noah/Pretrained-Language-Model.svg?style=social&label=Github%20%20Star)](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT) | `Extreme` |
| 2020.09.17 | 2020.09.18   |                      | Towards Fully 8-bit Integer Inference for the Transformer Model | [![arXiv](https://img.shields.io/badge/arXiv-2009.08034-b31b1b.svg)](https://arxiv.org/abs/2009.08034) |           |
| 2020.09.16 | 2020.10.13   |                      | Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation | [![arXiv](https://img.shields.io/badge/arXiv-2009.07453-b31b1b.svg)](https://arxiv.org/abs/2009.07453) |           |
| 2020.05.08 | 2020.09.27   | GOBO                 | Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference | [![arXiv](https://img.shields.io/badge/arXiv-2005.03842-b31b1b.svg)](https://arxiv.org/abs/2005.03842) |           |
| 2019.10.14 | 2019.10.17   | Q8BERT               | Quantized 8Bit BERT                                          | [![arXiv](https://img.shields.io/badge/arXiv-1910.06188-b31b1b.svg)](https://arxiv.org/abs/1910.06188) [![Github stars](https://img.shields.io/github/stars/intellabs/model-compression-research-package.svg?style=social&label=Github%20%20Star)](https://github.com/intellabs/model-compression-research-package) |           |
| 2019.09.12 | 2019.09.25   | Q-BERT               | Hessian Based Ultra Low Precision Quantization of BERT       | [![arXiv](https://img.shields.io/badge/arXiv-1909.05840-b31b1b.svg)](https://arxiv.org/abs/1909.05840) |           |
| 2019.06.03 | 2019.06.07   |                      | Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model | [![arXiv](https://img.shields.io/badge/arXiv-1906.00532-b31b1b.svg)](https://arxiv.org/abs/1906.00532) |           |

- `Non-uniform`：非均匀量化
- `MP`：mixed-precision quantization，混合精度量化
- `Extreme`：binary or ternary quantization，二值、三值量化


**参考**

- https://github.com/Zhen-Dong/Awesome-Quantization-Papers
- https://github.com/cmhungsteve/Awesome-Transformer-Attention#model-compression--transformer